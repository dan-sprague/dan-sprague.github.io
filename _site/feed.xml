<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-11-11T18:52:36-05:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Dan Sprague</title><subtitle>Stories of variation around a mean.</subtitle><entry><title type="html">Bayesian Uncertainty Estimation for Small Sample LogNormal Data</title><link href="http://localhost:4000/2025/11/10/bayesian-lognormal" rel="alternate" type="text/html" title="Bayesian Uncertainty Estimation for Small Sample LogNormal Data" /><published>2025-11-10T23:00:00-05:00</published><updated>2025-11-10T23:00:00-05:00</updated><id>http://localhost:4000/2025/11/10/bayesian-lognormal</id><content type="html" xml:base="http://localhost:4000/2025/11/10/bayesian-lognormal"><![CDATA[<style>
/* Add responsive design styles */
img {
  max-width: 100%;
  height: auto;
}
.figure-container {
  margin-bottom: 20px;
  width: 100%;
}
</style>

<p><br /></p>

<p><strong>Key findings:</strong></p>

<ul>
  <li><strong>Appropriate Transformation</strong>: LogNormal distributed data must have uncertainty quantified in the log transformed space.</li>
  <li><strong>Bayesian Posterior Estimation</strong>: A statistical method is proposed that better estimates the underlying distirbution and uncertainty for lognormal data.</li>
  <li><strong>Regularization benefit</strong>: Common sense priors prevent extreme parameter estimates that commonly occur with small sample maximum likelihood estimation, particularly for scale parameters</li>
  <li><strong>Average KL divergence</strong>: Bayesian method caused large (approximately 90% reduction) and significant decrease in average KL divergence in simulation experiment compared against frequentist (maximum likelihood)</li>
  <li><strong>Consistent improvement</strong>: The Bayesian approach provides superior distribution approximation across diverse parameter regimes
<br /><br /></li>
</ul>
<hr />

<h2 id="overview">Overview</h2>

<p>Biological data often spans multiple orders of magnitude and is right-skewed away from the mean. Standard visualization practice transforms the data to log-space while adjusting axis labels to display the original scale (Figure 1). While this transformation aids visual interpretation, it introduces a subtle but important consideration for uncertainty estimation.</p>

<div class="figure-container">
  <img src="/assets/images/plot.svg" alt="Prior distribution comparisons" />
  <figcaption style="text-align: center; font-style: plain; font-size: 0.9em;">Figure 1: Simulated data often seen in biological contexts.</figcaption>
</div>

<p>Statistical inference for lognormal data must be conducted on the log-transformed scale where the parameters estimating the average then follow a normal distribution. Consider the two groups in Figure 1 with medians at 10 and 7000 in the original scale. Correct inference requires evaluating the difference $\log(7000) - \log(10) \approx 6.55 - 2.30 = 4.25$ on the log scale where the data are normally distributed and can be compared against a null standard normal. Moreover, naively applying standard statistical methods (like frequentist confidence intervals) directly to lognormal data can lead to inferential errors, as we demonstrate in the following section.</p>
<p><br /></p>

<p>A bayesian method is proposed that better estimates the uncertainty from log-normal data.</p>

<hr />

<h2 id="standard-ci-underestimate-uncertainty">Standard CI Underestimate Uncertainty</h2>

<p>Inferring 95% confidence intervals prior to log-transforming the data leads to incorrect 95% CI (Figure 2, left). This is shown that as the data becomes more skewed with $\sigma$, the odds that the 95% CI contains the true parameter value goes down. Even when using the correct approach to calculate SEM in the log scale (Figure 2, center), small sample confidence intervals fail to capture upper tail behavior (Figure 2, right). 
<br /><br />
To demonstrate these issues, 10,000 datasets were simulated for various sample sizes (n = 3, 5, 10, 20, 50) and scale parameters (σ = 0.25, 0.5, 1.0, 2.0). For each dataset, we computed 95% confidence intervals using the standard $\bar{x} \pm t_{\alpha/2, n-1} \cdot \text{SEM}$ formula on both scales.</p>

<div class="figure-container">
  <img src="/assets/images/coverage_combined.svg" alt="Coverage probability comparison" />
  <figcaption style="text-align: left; font-style: plain; font-size: 0.9em;">Figure 2: Three problems with frequentist confidence intervals for lognormal data. Left:  Estimated 95% CIs fail to contain the true mean 95% of the time when CI is computed on original scale. Middle: Correct nominal coverage when CI is computed on log scale. Right: Even with correct log-scale CIs, the upper bound systematically falls below the true 95th percentile.</figcaption>
</div>

<p>Taken together, these graphs illustrate two things: Sampling error must be considered on the transformed space, and that standard frequentist confidence bands result in inferior understanding of the underlying data distribution.</p>

<h3 id="implication">Implication</h3>
<p>For lognormal biological data where upper tail behavior matters (e.g., maximum drug concentrations, peak immune responses), standard frequentist confidence intervals are inadequate. Researchers need methods that properly quantify uncertainty about tail behavior.</p>
<hr />

<h2 id="model-specification">Model Specification</h2>

<p>Bayesian models allow us to place common sense priors against the data that prevent model overfitting, particularly in the case of small data. In the case of the data shown in Figure 1, two important priors admit themselves.</p>

<ol>
  <li>The group means are, a priori, equally likely between $y_{min}$ and $y_{max}$ and follow a T-Distribution due to the small sample size.</li>
  <li>The scale of the log normal is, a priori, likely to be close to standard.</li>
</ol>

<p><strong>Mean Parameters</strong>: The group mean parameters are modeled as shifted T-Distributions, with locations $\mu_j$ given uniform priors $\nu$ over the observed log-data range, while the degress of freedom parameter $\tau$ is estimated from the data.</p>

<p><strong>Scale Parameters</strong>: The group scale parameters $\sigma_j$ use a Gamma prior parameterized such that the mode is precisely equal to 1. This is derived with $\alpha = (1/\beta) + 1$, where $\beta \sim \text{Exponential}(1)$. This enforces a prior for standard scale while allowing the model to estimate uncertainty in the scale parameter. The gamma distribution was chosen because the exponential distribution either has a mode at 0 (Figure 3, blue) which is not our prior belief, or a mean that isn’t 1 (red).</p>

<div class="figure-container">
  <img src="/assets/images/prior_comparison.svg" alt="Prior distribution comparisons" />
  <figcaption style="text-align: center; font-style: plain; font-size: 0.9em;">Figure 3: Comparison of prior distributions for location and scale parameters across groups.</figcaption>
</div>

<p>The unique parameterization of the Gamma distribution represents the ideal behavior for the scale prior. The posterior for each group is the estimated using the data.</p>

<hr />

<h3 id="probabilistic-model">Probabilistic Model</h3>

<p>The model is specified as follows:</p>

<table style="width: 100%; border-collapse: collapse;">
<tr style="border-bottom: 1px solid #ddd;">
  <td style="padding: 10px; vertical-align: top; width: 30%;"><strong>T-Distributed Means</strong></td>
  <td style="padding: 10px; vertical-align: top;">
<div style="text-align: left; display: inline-block;">
$$
\begin{aligned}
\tau &amp;\sim \text{LocationScale}(1, 1, \text{Exponential}(1/29)) \\
\nu &amp;\sim \text{Uniform}(\min(\log y), \max(\log y)) \quad \text{for } j = 1,\ldots,6 \\
\mu_j &amp;\sim \text{LocationScale}(\nu, 1, \text{TDist}(\tau)) \quad \text{for } j = 1,\ldots,6 \\
\end{aligned}
$$
</div>
  </td>
</tr>
<tr style="border-bottom: 1px solid #ddd;">
  <td style="padding: 10px; vertical-align: top;"><strong>Standard scale prior</strong></td>
  <td style="padding: 10px; vertical-align: top;">
<div style="text-align: left; display: inline-block;">
$$
\begin{aligned}
\beta &amp;\sim \text{Exponential}(1) \\
\alpha &amp;= \frac{1}{\beta} + 1 \\
\sigma_j &amp;\sim \text{Gamma}(\alpha, \beta) \quad \text{for } j = 1,\ldots,6 \\
\end{aligned}
$$
</div>
  </td>
</tr>
<tr>
  <td style="padding: 10px; vertical-align: top;"><strong>Likelihood</strong></td>
  <td style="padding: 10px; vertical-align: top;">
<div style="text-align: left; display: inline-block;">
$$
\begin{aligned}
y_i &amp;\sim \text{LogNormal}(\mu_{c_i}, \sigma_{c_i})
\end{aligned}
$$
</div>
  </td>
</tr>
</table>

<p>where $c_i$ denotes the class (group) assignment for observation $i$.</p>

<h2 id="julia-implementation">Julia Implementation</h2>

<p>The Turing.jl package enables easy specification and fast sampling of this relatively simple model.</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">using</span> <span class="n">Turing</span>
<span class="k">using</span> <span class="n">Distributions</span>

<span class="nd">@model</span> <span class="k">function</span><span class="nf"> lognormal_model</span><span class="x">(</span><span class="n">class</span><span class="x">,</span> <span class="n">y</span><span class="x">)</span>
    <span class="c"># Number of unique groups (6 total)</span>
    <span class="n">n_groups</span> <span class="o">=</span> <span class="n">length</span><span class="x">(</span><span class="n">unique</span><span class="x">(</span><span class="n">class</span><span class="x">))</span>

    <span class="c"># Data range for uniform prior on location</span>
    <span class="n">y_min</span> <span class="o">=</span> <span class="n">minimum</span><span class="x">(</span><span class="n">log</span><span class="o">.</span><span class="x">(</span><span class="n">y</span><span class="x">))</span>
    <span class="n">y_max</span> <span class="o">=</span> <span class="n">maximum</span><span class="x">(</span><span class="n">log</span><span class="o">.</span><span class="x">(</span><span class="n">y</span><span class="x">))</span>

    <span class="c"># Degrees of freedom for t-distribution</span>
    <span class="n">τ</span> <span class="o">~</span> <span class="n">LocationScale</span><span class="x">(</span><span class="mi">1</span><span class="x">,</span> <span class="mi">1</span><span class="x">,</span> <span class="n">Exponential</span><span class="x">(</span><span class="mi">1</span><span class="o">/</span><span class="mi">29</span><span class="x">))</span>

    <span class="c"># Priors on location parameters - uniform from min to max of log(y)</span>
    <span class="n">ν</span> <span class="o">~</span> <span class="n">filldist</span><span class="x">(</span><span class="n">Uniform</span><span class="x">(</span><span class="n">y_min</span><span class="x">,</span> <span class="n">y_max</span><span class="x">),</span> <span class="n">n_groups</span><span class="x">)</span>
    <span class="n">μ</span> <span class="o">~</span> <span class="n">arraydist</span><span class="x">(</span><span class="n">LocationScale</span><span class="o">.</span><span class="x">(</span><span class="n">ν</span><span class="x">,</span> <span class="mf">1.0</span><span class="x">,</span> <span class="n">TDist</span><span class="x">(</span><span class="n">τ</span><span class="x">)))</span>

    <span class="c"># Mode = 1 prior for standard lognormal scale</span>
    <span class="n">β</span> <span class="o">~</span> <span class="n">Exponential</span><span class="x">()</span>
    <span class="n">α</span> <span class="o">=</span> <span class="x">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">β</span><span class="x">)</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">σ</span> <span class="o">~</span> <span class="n">filldist</span><span class="x">(</span><span class="n">Gamma</span><span class="x">(</span><span class="n">α</span><span class="x">,</span><span class="n">β</span><span class="x">),</span> <span class="n">n_groups</span><span class="x">)</span>

    <span class="n">y</span> <span class="o">~</span> <span class="n">product_distribution</span><span class="x">(</span><span class="n">LogNormal</span><span class="o">.</span><span class="x">(</span><span class="n">μ</span><span class="x">[</span><span class="n">class</span><span class="x">],</span> <span class="n">σ</span><span class="x">[</span><span class="n">class</span><span class="x">]))</span>
<span class="k">end</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">lognormal_model</span><span class="x">(</span><span class="n">class_data</span><span class="x">,</span> <span class="n">y_data</span><span class="x">)</span>
<span class="n">chain</span> <span class="o">=</span> <span class="n">sample</span><span class="x">(</span><span class="n">model</span><span class="x">,</span> <span class="n">NUTS</span><span class="x">(),</span> <span class="mi">5000</span><span class="x">;</span><span class="n">drop_warmup</span><span class="o">=</span><span class="nb">true</span><span class="x">)</span>
</code></pre></div></div>

<p><br /><br /></p>
<h1 id="results">Results</h1>

<p><strong>Comparing 95%CI</strong></p>
<div class="figure-container">
  <img src="/assets/images/comparison_plot.svg" alt="Model fit comparison" />
  <figcaption style="text-align: center; font-style: plain; font-size: 0.9em;">Figure 4: Posterior predictive distributions compared to observed data for each group.</figcaption>
</div>

<p><strong>Evaluating fitted distribution against simulated ground truth</strong></p>

<p>To quantify the improvement of the hierarchical Bayesian approach over standard maximum likelihood estimation across different levels of data skewness, we simulated 30 groups with n=3 samples for each of four scale parameters (σ = 0.25, 0.5, 1.0, 2.0). For each group, we calculated the Kullback-Leibler (KL) divergence between each method’s estimated distributions and the true underlying distributions. Lower KL divergence indicates better approximation of the true distribution.</p>

<div class="figure-container">
  <img src="/assets/images/kl_divergence_comparison.svg" alt="KL divergence comparison" />
  <figcaption style="text-align: center; font-style: plain; font-size: 0.9em;">Figure 5: Comparison of average KL divergence between maximum likelihood (red) and Bayesian hierarchical (blue) models across different scale parameters. The Bayesian approach consistently outperforms ML across all levels of data skewness, with ML showing high variance while Bayesian maintains stable, low KL divergence.</figcaption>
</div>

<hr />

<h2 id="discussion">Discussion</h2>

<p>The hierarchical structure allows for partial pooling of information across groups while maintaining group-specific estimates. The uniform prior on location parameters bounded by the observed data range provides a weakly informative prior that regularizes extreme estimates without imposing strong assumptions.</p>

<p>The mode-at-one parameterization for the scale parameter represents a principled choice for lognormal data, as it centers the prior on a neutral scaling assumption while allowing the data to drive the posterior away from this default when warranted.</p>

<p>This substantial improvement in KL divergence demonstrates that the Bayesian hierarchical approach provides superior approximation of the true underlying distributions, especially in small-sample scenarios where maximum likelihood estimation is unstable.</p>

<hr />

<h2 id="methods">Methods</h2>

<p>Analysis was performed using <a href="https://turing.ml/">Turing.jl</a> for probabilistic programming. Code and data are available at [link to repository].</p>

<h3 id="inference-details">Inference Details</h3>

<ul>
  <li>Sampler: NUTS with automatic differentiation</li>
  <li>Chains: 4 independent chains</li>
  <li>Iterations: 2000 per chain (1000 warmup)</li>
  <li>Convergence diagnostics: $\hat{R} &lt; 1.01$ for all parameters</li>
</ul>

<hr />]]></content><author><name></name></author><category term="Statistical Modeling" /><category term="Julia" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Neighborhood Conservation Districts Suppress Housing in Cambridge, MA</title><link href="http://localhost:4000/2025/03/30/ncd" rel="alternate" type="text/html" title="Neighborhood Conservation Districts Suppress Housing in Cambridge, MA" /><published>2025-03-30T00:00:00-04:00</published><updated>2025-03-30T00:00:00-04:00</updated><id>http://localhost:4000/2025/03/30/ncd</id><content type="html" xml:base="http://localhost:4000/2025/03/30/ncd"><![CDATA[<style>
/* Add responsive design styles */
img {
  max-width: 100%;
  height: auto;
}
.figure-container {
  margin-bottom: 20px;
  width: 100%;
}
.table-container {
  overflow-x: auto;
  margin-bottom: 20px;
}
@media (min-width: 768px) {
  .flex-container {
    display: flex;
    gap: 20px;
    align-items: flex-start;
  }
  .flex-item-large {
    flex: 2;
    min-width: 0;
  }
  .flex-item-small {
    flex: 1;
    min-width: 0;
  }
}
</style>

<hr />

<h3 id="tldr">tl;dr</h3>

<ol>
  <li>Analyses were performed by controlling for area, base zoning, and neighborhood.</li>
  <li>NCDs rarely issue even a single permit that has a net housing increase of at least 1 unit in an entire year.</li>
  <li>⁠The Mid-Cambridge NCD is ~50% less likely to issue a single housing-positive permit than an equivalent Res C plot of land outside the NCD per year.</li>
  <li>The Half Crown-Marsh NCD stands in a world of its own, having issued no housing positive permits in the 29 year dataset.</li>
  <li>NCDs are hotspots for down-conversion projects, and with more housing losses than comparable plots of land outside the NCD.</li>
  <li>For projects that are net housing losses, NCDs are associated with a ~10 fold increase in the number of units lost, on average, compared to an equivalent Res C plot of land outside the NCD.</li>
</ol>

<hr />

<p><br /></p>

<h2 id="background">Background</h2>
<p>Cambridge is a small city immediately adjacent to Boston, Massachusetts with one of the worst housing crises in the country. Despite this, the city has done remarkably little to address the problem. Inside the city’s residential districts, the city constructed a <em>cumulative</em> 1,848 housing units over 29 years from 1996 to 2024, which amounts to 0.06 housing units per 100 residents per year. Notably, Cambridge has found a way to build most of its residences in non-residential districts. Of the total 14,216 net residences constructed since 1996, 12,368 were constructed in non-residential areas of the city. Annual housing production per sqkm is shown in Fig 1 below.<br /></p>

<div class="figure-container" style="max-width: 400px; margin: 0 auto; margin-bottom: 10px;">
  <img src="/assets/images/housing_area.jpg" alt="Housing per sqkm" />
  <figcaption style="text-align: center; font-style: plain; font-size: 0.9em;">Fig. 1 Housing units per square kilometer. The focus of this analysis will be all housing that falls within the red ("In Residential") category.</figcaption>
</div>

<p>Cambridge’s residential areas, importantly, are not near capacity. These areas contain large numbers of detached 1-2 family housing units, some on estate sized lots that would not be out-of-place in the exurbs. The city has repeatedly found that there is adequate resources and infrastructure to build more housing in these areas. The question is not if there should be development but why there isn’t development in such a supply constrained market.<br /></p>

<p>In 2025, the Cambridge city council passed one of the most sincere zoning reforms in the United States. With this accomplishment, the city government not only increased the allowable housing density across large regions of Cambridge, it also corrected the historical injustice of applying different housing rules to different neighborhoods with absolutely no underlying environmental or infrastructure based justification. This ordinance makes serious attempts at not just changing the base zoning, but also relieving arbitrary and punitive dimensional constraints which serve little purpose other than to restrict housing construction.<br /></p>

<h3 id="neighborhood-conservation-districts-ncds">Neighborhood Conservation Districts (NCDs)</h3>
<p>NCDs were established by the city council in 1983 in an effort to protect the character of wealthy neighborhoods in the city from further development. According to the Cambridge Historical Commission’s (CHC) website, these districts were founded to be “more effective” than zoning and more flexible than historic districts. Perhaps sensing that elected officials are suspicious, CHC goes to great length on their official website to cite the number of applications approved by the city. For example, CHC cites that 462 applications were approved in the Half Crown-Marsh NCD since 2013 for a 96.9% approval rate. Interesting that those 462 applications has resulted in -6 housing units within the NCD (Table 1, below).<br /></p>

<div class="flex-container">
  <div class="flex-item-large figure-container">
    <img src="/assets/images/zoning_ncd_overlapy.png" alt="Zoning map with NCDs" />
    <figcaption style="text-align: center; font-size: 0.9em;">Fig. 2 Historical residential zoning map of Cambridge MA showing regions of the city zoned for residential use (Red/Blue/Green), with areas of the city designated as special Neighborhood Conservation Districts (NCDs) shaded in.</figcaption>
  </div>
  
  <div class="flex-item-small">
    <div class="table-container">
      <table style="font-size: 0.85em; width: 100%;">
        <thead>
          <tr>
            <th style="white-space: normal;">Neighborhood Conservation District</th>
            <th style="text-align: right;">Total Net Housing Change (1996-2024)</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td style="white-space: normal;">None (Non-NCD Areas)</td>
            <td style="text-align: right;">1862</td>
          </tr>
          <tr>
            <td style="white-space: normal;">Avon Hill</td>
            <td style="text-align: right;">-5</td>
          </tr>
          <tr>
            <td style="white-space: normal;">Half Crown-Marsh</td>
            <td style="text-align: right;">-6</td>
          </tr>
          <tr>
            <td style="white-space: normal;">Harvard Square</td>
            <td style="text-align: right;">-6</td>
          </tr>
          <tr>
            <td style="white-space: normal;">Mid Cambridge</td>
            <td style="text-align: right;">3</td>
          </tr>
        </tbody>
      </table>
      <figcaption style="text-align: center; font-size: 0.9em;">Table 1: Net Housing Change by NCD Status, not adjusted for area.</figcaption>
    </div>
  </div>
</div>

<p>When considering the impact of NCDs, it is important to consider what they are <em>not</em> allowed to do:<br /></p>

<ol>
  <li>NCDs are not allowed to impose dimensional or setback restrictions greater than the base zoning</li>
  <li>Cannot consider size and shape (“massing”)</li>
  <li>Limited to considering incongruity within the district
<br /></li>
</ol>

<p>In spite of this, NCDs have the power to issue binding decisions that can prevent permit issuance for a project with almost no oversight from the city council. Given that NCDs are forbidden from flouting the base zoning(s) of the district they reside within, if CHC’s claim is true that they issue approval 96.9% of applications then their should not be statistically meaningful differences in housing permits after controlling for neighborhood, base zoning(s), and area.
<br /></p>

<p>To determine if this is true, I examined the citywide dataset of housing starts and found statistically significant evidence that NCDs act as an independent suppressor on housing in Cambridge. The raw data says enough on its own: housing production in Avon Hill, Half Crown-Marsh, and Harvard Square has been <em>negative</em> over the past 29 years. In the same time span, the Mid Cambridge NCD has managed to add 3 housing units inside an area that is nearly a square kilometer (Table 1). <br /></p>

<p>This is an important problem because the city recently upzoned all A and B regions of the city to C. If NCDs suppress housing, this will diminish the impact of the law.<br /></p>

<h2 id="results">Results</h2>
<p>One of the more remarkable aspects of the housing starts dataset (outlined more in the Methods section below) is that Cambridge simply builds almost no housing, inside or outside of NCDs. This makes separating the effect of NCDs from base zoning a challenging task, particularly when considering NCDs that are a fraction of a square kilometer. The relevant statistical question, therefore, is pretty straightfoward: How many permits for how many units would be issued per sqkm per year in Res C, and does the observed data suggest a difference for areas inside NCDs?<br /></p>

<p>Interestingly, <em>when</em> there is a housing positive development approved inside an NCD there is no statistical evidence that the number of units added is different from an equally zoned region outside the NCD (mean NCD fold change = 1.03 +/- 0.52).<br /></p>

<h4 id="ncds-issue-fewer-housing-positive-permits-than-other-equal-areaequal-zoned-regions">NCDs Issue Fewer Housing Positive Permits Than Other Equal Area/Equal Zoned Regions</h4>

<p>There is a large difference in the number of years for which their was <em>at least 1</em> (yes just 1) net housing increase permit issued between NCDs and not-NCD areas. The results are striking: In nearly all cases, NCDs have substantially more years without <em>any</em> housing positive permits issued (Figure 3). Mid-Cambridge, the largest NCD with the most available data, gives the clearest picture of the effect that an NCD has when nested inside a predominantly Res C zoning district. The Mid-Cambridge NCD is ~50% less likely to issue a <em>single</em> permit that adds at least one net housing unit in a given year than an equivalent Res C plot of land outside the NCD. Half Crown-Marsh stands in a world of its own, issuing essentially nothing in the 29 year observation period.<br /></p>

<div class="figure-container">
  <img src="/assets/images/permits.png" alt="Permit probability by zone" />
  <figcaption style="text-align: center; font-size: 0.9em;">Fig. 3 Probability of issuing at least one housing-increase permit annually, by zoning type and NCD status. Adjusted for area.</figcaption>
</div>

<h4 id="ncds-are-down-conversion-hotspots">NCDs Are Down-conversion Hotspots</h4>

<p>Perhaps more concerning than the rarity of housing unit increases is the concentration of down-conversions inside NCDs as compared to the remainder of the city. Indeed, after splitting the data into permits with net removals or net additions and examining the removals in isolation it is clear that NCDs have a strong and statistically significant effect that is associated with a large multiplicative increase (mean = 10.69x, 95% CI [4.50,21.57]) in the average number of unit losses per sqkm per year relative to the base rate of unit losses in an equally sized Res C parcel (Figure 4).</p>

<div class="figure-container">
  <img src="/assets/images/map_removals.png" alt="Zoning map with NCDs" />
  <figcaption style="text-align: center; font-size: 0.9em;">Fig. 4 Regression estimates for annual housing unit losses per sqkm, controlled for base zoning and neighborhood. Inset shows the unit loss fold change associated with a project being in Res. A, Res.B, or in an NCD relative to the base unit loss rate in Res C. Bracketed numbers indicate lower and upper 95%CI. Green lines demarcate NCD regions.</figcaption>
</div>

<p>This is strong evidence that NCDs preferentially approve permits that are net housing losses. It is also reflective of the very real chilling effect that NCDs have on developers: why spend months battling against an unaccountable group and endless carrying costs when the lot can simply be converted into a single family residence?</p>

<hr />

<h1 id="methods">Methods</h1>

<p>R code and data are available at my <a href="https://github.com/dan-sprague/CambridgeHousing"><u>github repo for this project</u></a>.</p>

<h2 id="data-sources-and-preparation">Data Sources and Preparation</h2>

<p>Data sources included GeoJSON files of NCD boundaries, residential districts, zoning districts, and neighborhood boundaries, alongside housing starts from 1996 to present, containing building permit and housing unit change data. Area calculations for each spatial unit were calculated in square kilometers. Zoning district areas were adjusted to account for NCD overlaps to ensure accurate density measurements. 
<br />
Two specific cases inside the data were adjusted to correct for errors:</p>

<ol>
  <li>125 Harvard St
    <ul>
      <li>The dataset incorrectly attributed the Geocode address to 345 Harvard St, which falls within the Mid Cambridge NCD.</li>
      <li>The correct lat and lon were added for this entry.</li>
    </ul>
  </li>
  <li>273 Harvard St
    <ul>
      <li>This development converted a nursing home into a assissted living for senior facility. Records do not exist for the number of beds present at this location prior to its reconstruction.</li>
      <li>The development was removed from the dataset.</li>
    </ul>
  </li>
</ol>

<h2 id="spatial-analysis">Spatial Analysis</h2>

<p>Spatial joins connected housing data with corresponding geographic areas. Each development was assigned attributes indicating NCD status, specific NCD name (if applicable), residential district, zoning type (A, B, or C), and neighborhood name.</p>

<p>Housing data was filtered to exclude entries with missing coordinates or undefined net unit changes. Data was aggregated by year and geography, with zero values included for years without recorded housing activity for each unique combination of NCD, Neighborhood, Assessing District, and Zoning Area.</p>

<h2 id="metrics">Metrics</h2>

<p>Many projects have negative net unit swings, which cannot be directly modeled by count distributions most appropriate for this data. For this reason, the data were split into three categories:</p>

<ol>
  <li>Housing additions (positive net change)</li>
  <li>Housing removals (absolute value of negative net change)</li>
  <li>Housing starts (permits with positive net change per year)</li>
</ol>

<p>All metrics were normalized by geographic area for valid cross-district comparisons.</p>

<h2 id="statistical-modeling">Statistical Modeling</h2>

<p>Bayesian generalized linear models (GLMs) were used to fit the relationship between NCDs and housing development while controlling for other factors. Due to the preponderance of zero observations and high variance of unit changes, zero-inflated negative binomial likelihoods were used for the housing additions and removals. For the permit starts data, a beta-binomial likelihood was used to account for the relatively large amount of uncertainty present in the data. Together, these models were chosen to best estimate uncertainty and minimize unsupported conclusions from the data.</p>

<ol>
  <li><strong>Housing Additions Model</strong>: Zero-inflated negative binomial model:
    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model_additions</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">brm</span><span class="p">(</span><span class="w">
  </span><span class="n">housing_added</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">Zoning</span><span class="w"> </span><span class="o">+</span><span class="w">  </span><span class="n">IsNCD</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="p">(</span><span class="m">1</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">Year.Permitted</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="p">(</span><span class="m">1</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">Neighborhood_name</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">offset</span><span class="p">(</span><span class="nf">log</span><span class="p">(</span><span class="n">area</span><span class="p">)),</span><span class="w">
  </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">modeling_data</span><span class="p">,</span><span class="w">
  </span><span class="n">family</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">zero_inflated_negbinomial</span><span class="p">(),</span><span class="w">
  </span><span class="n">chains</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">8</span><span class="p">,</span><span class="w"> 
  </span><span class="n">iter</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1000</span><span class="p">,</span><span class="w">
  </span><span class="n">cores</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">8</span><span class="p">)</span><span class="w">
</span></code></pre></div>    </div>
  </li>
  <li><strong>Housing Removals Model</strong>: Zero-inflated negative binomial model:
    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="w"> </span><span class="n">model_removals</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">brm</span><span class="p">(</span><span class="w">
  </span><span class="n">housing_removed</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">IsNCD</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">Zoning</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="p">(</span><span class="m">1</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">Year.Permitted</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">  </span><span class="p">(</span><span class="m">1</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">Neighborhood_name</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">offset</span><span class="p">(</span><span class="nf">log</span><span class="p">(</span><span class="n">area</span><span class="p">)),</span><span class="w">
  </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">modeling_data</span><span class="p">,</span><span class="w">
  </span><span class="n">family</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">zero_inflated_negbinomial</span><span class="p">(),</span><span class="w">
  </span><span class="n">chains</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">8</span><span class="p">,</span><span class="w"> 
  </span><span class="n">iter</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2000</span><span class="p">,</span><span class="w">
  </span><span class="n">cores</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">8</span><span class="w">
</span><span class="p">)</span><span class="w">
</span></code></pre></div>    </div>
  </li>
  <li><strong>Permit Probability Model</strong>: Beta-binomial model:
    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="w">  </span><span class="n">permit_model</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">brm</span><span class="p">(</span><span class="w">
  </span><span class="n">bf</span><span class="p">(</span><span class="n">successes</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">trials</span><span class="p">(</span><span class="n">successes</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">failures</span><span class="p">)</span><span class="w"> </span><span class="o">~</span><span class="w">  </span><span class="n">Zoning</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">NCD</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">offset</span><span class="p">(</span><span class="nf">log</span><span class="p">(</span><span class="n">area</span><span class="p">))),</span><span class="w">
  </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">corrected_permit_data</span><span class="p">,</span><span class="w">
  </span><span class="n">family</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">beta_binomial</span><span class="p">(),</span><span class="w">
</span><span class="n">prior</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="w">
  </span><span class="n">prior</span><span class="p">(</span><span class="n">normal</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="p">),</span><span class="w"> </span><span class="n">class</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"b"</span><span class="p">),</span><span class="w">
  </span><span class="n">prior</span><span class="p">(</span><span class="nf">gamma</span><span class="p">(</span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="p">),</span><span class="w"> </span><span class="n">class</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"phi"</span><span class="p">)</span><span class="w">  </span><span class="c1"># Prior for overdispersion</span><span class="w">
</span><span class="p">),</span><span class="w">
  </span><span class="n">chains</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">8</span><span class="p">,</span><span class="w">
  </span><span class="n">cores</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">8</span><span class="p">,</span><span class="w">
  </span><span class="n">iter</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2000</span><span class="p">)</span><span class="w">
</span></code></pre></div>    </div>
  </li>
</ol>

<p>Models incorporated area offsets and random effects for year and neighborhood to account for size differences, temporal trends, and geographic variations.</p>

<h2 id="implementation-and-analysis">Implementation and Analysis</h2>

<p>Models were implemented using the <code class="language-plaintext highlighter-rouge">brms</code> package with 8 MCMC chains and 1,000-2,000 iterations. Weakly informative priors were specified for the permit probability model to improve stability.</p>

<p>Analyses derived from posterior distributions included effects of NCDs on housing additions/removals, permit issuance probability estimates across zoning types and NCD status, and 95% credible intervals for all parameters.</p>]]></content><author><name></name></author><category term="Statistical Modeling" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Thinking Machines</title><link href="http://localhost:4000/2025/02/01/llmthinking" rel="alternate" type="text/html" title="Thinking Machines" /><published>2025-02-01T23:00:00-05:00</published><updated>2025-02-01T23:00:00-05:00</updated><id>http://localhost:4000/2025/02/01/llmthinking</id><content type="html" xml:base="http://localhost:4000/2025/02/01/llmthinking"><![CDATA[<h2 id="table-of-contents">Table of Contents</h2>
<ul>
  <li><a href="#language-models">Language Models</a></li>
  <li><a href="#llms-are-approximating-densities-of-natural-language">LLMs Are Approximating Densities of Natural Language</a></li>
</ul>
<hr />

<p><br /></p>

<p>To think before one speaks is to consider a set of possible responses and choose the one that best conveys one’s intent. This is a task that most humans do intuitively and quickly, but is not something that is easily incorporated into modern AI systems. Compared to the intricacy of large language model (LLM) architectures, the methods used to sample responses from an LLM, given a prompt, are simple and heuristic driven.<br /><br /></p>

<p>Given a prompt by the user, the LLM first decides which word out of all words in the dictionary should come first in its response. This decision is made by choosing the word with the highest estimated likelihood, given the model’s training and the context provided in the form of a prompt. Note that at this point, the LLM has no information about the rest of its response to weigh in on its word choice.<br /><br /></p>

<p>Crucially, this initial word choice may end up being a mistake in the long run because the final likelihood of the generated response may have been higher if a different initial word had been chosen. This problem compounds with each word choice as sentences and paragraphs are constructed by the model. Unfortunately, it is impossible for LLMs to exhaustively search for the best response to a query. The reason for this is practical: to guarantee the best possible response to a prompt would require scoring $L^V$ responses, where $L$ is the length of the response and $V$ is the size of the vocabulary. For human language, $V$ is on the order of $10^4$ and response lengths can be hundreds of words long. Clearly, this computational problem is intractable. The practical outcome is that it is extremely unlikely, if not almost guaranteed, that the response from the LLM is not the best possible response it could have given.<br /><br /></p>

<p>It is interesting that LLMs work as well as they do despite the relative simplicity of their sequence generation. This may be one reason why LLMs require such a large amount of training data relative to humans to generate meaningful language. Fortunately, LLMs contain additional information that can be used to generate responses more efficiently. The LLM’s gradient, like the slope of a line, points in the direction of higher scoring response. This means that it is not necessary to move blindly through the response search space, like the greedy heuristic. Rather, the gradient can be used to sample better responses with much higher probability than otherwise. It is not known if using gradient information from a set of initial heuristic samples meaningfully improves LLM responses. This article attempts to address that question to the extent that a lone Apple iMac is capable. To do this, two questions need to be addressed:
<br /><br /></p>

<ol>
  <li>To what extent are higher likelihood language samples associated with more nuanced, complex responses?</li>
  <li>Can the gradient of the LLM’s log likelihood, $\nabla \log{\hat{p}(x \mid \theta, c)}$, otherwise known as the score function, be used to generate a set of higher likelihood samples?
<br /><br /></li>
</ol>

<h2 id="language-models">Language Models</h2>
<p>As far as one believes that humans convey their reasoning and knowledge through language, and as far as one believes that human language can be approximated by an arbitrarily complex statistical model, then we can propose that there is a probability distribution over all possible sequences of words $x = (x_1,\dots,x_n)$ that humans are likely to generate.</p>

\[p(x)\]

<p>Of course, the “true” nature of human language $p(x)$ is unknown and therefore requires mathematical approximation. Enter LLMs. The billions of weights inside an LLM encode correlations between words, sentences, and paragraphs. However, these weights represent only a static set of variables that are not capable of generating a sequence on their own. A method is required to “decode” the weights inside the model into a generated sequence given contextualizing information $c$, such as a prompt provided to the LLM from a human. In other words, a method is required to roll metaphorical dice to generate a sequence $x$ with probability $p$, given context $c$.</p>

\[x \mid c \sim p(x)\]

<p>The challenge LLMs tackle is twofold: estimating $p(x)$ and then sampling (generating) a reply to a prompt $c$ that maximizes $p(x \mid c)$.<br /><br /></p>

<h2 id="estimating-px-with-an-llm">Estimating p(x) with an LLM</h2>
<p>LLMs represent a transformational improvement in our ability to estimate the likelihood of human language. An LLM is a function $f$ parameterised by $\theta$, $f_\theta$, that consumes context $c$ and estimates the probability distribution over the vocabulary at each position $i$ within the generated sequence. For this reason, an LLM can be viewed as an approximating density for $p(x \mid c) \approx \hat{p}(x \mid \theta,c)$. The best response $x_{\texttt{best}}$ from the LLM is then the point of highest conditional density $\hat{p}(x \mid c,\theta)$, which directly means that $x_{\texttt{best}}$ must maximize $f_\theta$.</p>

\[p(x \mid c) \approx \hat{p}(x \mid \theta,c)\]

\[\hat{p}(x \mid \theta,c) = f_\theta(x)\]

\[x_{\text{best}} = \underset{x}{\operatorname{argmax}} f_\theta(x)\]

<p>The problem encountered by modern LLMs is that building a very good likelihood estimator was only half the problem. The other half is to sample the best response given context to the model. This is equivalent to finding the maximum of the likelihood function $f_{\theta}$. However, this requires checking all $L^V$ responses. As discussed above, this is an intractable problem.<br /><br /></p>

<h2 id="peak-finding-vs-peak-mapping">Peak Finding vs. Peak Mapping</h2>
<p>Imagine you have decided to drop yourself at a random location in the middle of Himalayas. Your goal is to find the highest peak, Everest, by only walking upwards. Crucially, once any peak in the mountain range is reached, you can no longer climb upwards so the task must complete. There are 3,411 named peaks in the Himalayas, of which only 1 is Mt. Everest. Clearly it is quite improbable to find Mt. Everest using this algorithm. This algorithm is known as gradient ascent, and it is related to how LLMs are trained.
<img src="/assets/images/himalayas.jpeg" height="300" alt="description" />
| Figure 1. Zoomed in area around Mt. Everest in the Himalayas.|
<br /><br /></p>

<p>Now imagine that you once again find yourself in the middle of the Himalayas. Rather than only climbing until a peak is reached, you continuously move around in a way that respects these rules:
<br /><br /></p>

<ol>
  <li>As you move uphill, you lose momentum and tend to make smaller moves</li>
  <li>As you move downhill, you gain momentum and start making larger moves</li>
  <li>In flat regions, your momentum doesn’t change and you make large moves until you either start climbing again (1) or descending again (2)</li>
  <li>You only stop moving after a pre-determined number of steps.
<br /><br /></li>
</ol>

<p>If this is done with an element of randomness, you will provably end up visiting peaks in the Himalayas proportionally to how high they are – virtually ensuring that you will eventually find the peak of Mt. Everest.</p>

<p><img src="/assets/images/path_opt.png" alt="Finding the best generated response" /></p>
<p align="justify">Figure 2. Gradient based generated sequence optimization. Left: Given an initial prediction from the model, the gradient of the LLM $\nabla f_\theta$ points the next prediction in a direction that is guaranteed to give a higher likelihood response, however these methods get trapped in local minima. Right: Gradient-based monte carlo samplers such as HMC use the gradient of the LLM $\nabla f_\theta$ to draw samples from $f_\theta$ proportionally to how likely the samples are from the model.</p>

<p><br /><br /></p>

<p>Mathematically, these algorithms can be expressed the following way and be used to generate the plots in Fig. 2:</p>

<p>Gradient Ascent<br /></p>

<p>$x_{i+1} \leftarrow x_i + η ∇f_θ(x_i)$</p>

<p>Gradient MCMC Samplers (HMC)<br /></p>

<p>$p_{i+1/2} \leftarrow p_i + (η/2) ∇f_θ(x_i)$</p>

<p>$x_{i+1} \leftarrow x_i + η p_{i+1/2}$</p>

<p>$p_{i+1} \leftarrow p_{i+1/2} + (η/2) ∇f_θ(x_{i+1})$</p>

<h2 id="thinking-like-an-llm">Thinking like an LLM</h2>

<p>Given these limitations, current LLM performance may be no where near its true level of knowledge or approximation of human language. It is possible that improvements to LLMs will come from a team of mathematicians and computer scientists in a few lines of math, rather than relying on exponentially more data. <br /></p>

<p>Our brains effortlessly sample language with essentially no error, especially on common knowledge subjects. The frequent and nonsensical “hallucinations” of an LLM are essentially a poor estimate for $\max{f_{LLM}}$ that a human brain would never make. <br /></p>

<p>As sampling algorithms for generative AI improves, we will get a better picture of the true level of knowledge store in modern LLMs. I strongly suspect that better exploration of the sequence space via improved markov-chain monte carlo methods or similar will result in improved AI performance despite the data ceiling.</p>

<h2 id="notes">Notes</h2>

<p>LLMs do not estimate the true likelihood of a sequence but rather the pseudolikelihood.</p>

<p>You generate an initial proposal response, but then reevaluate the response. You pass the initial response through the LLM which gives a probability/pseudolikelihood of the response. The calculation of that probability/pseudolikelihood is differentiable.</p>

<p>However my sampling choices at each position in the sequence are discrete, meaning they cannot be connected to this gradient. This is resolvable, potentially, by using a continuous relaxation calculation of the samples.</p>

<p>Initial → Continuous Relaxation → Gradient Updates → Final Argmax
tokens    (Gumbel-Softmax)       (using f_θ)        (discrete tokens)</p>

<p>From yang song</p>

<p>When sampling with Langevin dynamics, our initial sample is highly likely in low density regions when data reside in a high dimensional space. Therefore, having an inaccurate score-based model will derail Langevin dynamics from the very beginning of the procedure, preventing it from generating high quality samples that are representative of the data.</p>

<p>^ We solve this maybe by doing a greedy beam search then optimize</p>]]></content><author><name></name></author><category term="Statistical Modeling" /><summary type="html"><![CDATA[Table of Contents Language Models LLMs Are Approximating Densities of Natural Language]]></summary></entry><entry><title type="html">The Case for Julia in Computational Biology</title><link href="http://localhost:4000/2024/12/03/juliacase" rel="alternate" type="text/html" title="The Case for Julia in Computational Biology" /><published>2024-12-03T23:00:00-05:00</published><updated>2024-12-03T23:00:00-05:00</updated><id>http://localhost:4000/2024/12/03/juliacase</id><content type="html" xml:base="http://localhost:4000/2024/12/03/juliacase"><![CDATA[<p><br /></p>

<h2 id="table-of-contents">Table of Contents</h2>

<ul>
  <li><a href="#project-management-and-reproducibility">Project Management and Reproducibility</a></li>
  <li><a href="#text-processing">Text Processing</a></li>
  <li><a href="#fast-and-easy-multithreading">Fast and Easy Multithreading</a></li>
  <li><a href="#functions-are-center-stage">Functions Are Center Stage</a></li>
</ul>
<hr />

<p><br /></p>

<p>Why Julia? Python excels as a general purpose programming language and as a wrapper for neural network implementations, whereas R excels at data analysis, plotting, statistics, and for its robust library of bioinformatics packages. What is missing from the computational biology toolkit is a language that is easy to write, read, while maximizing performance on numerical and sequence based data. This is the niche that Julia fills.<br /><br /></p>

<h2 id="native-project-management-and-reproducibility">Native Project Management and Reproducibility</h2>
<p><br />
In the sciences, reproducibility and organization are perhaps the most important component of good research. This is generally true for most applications – which is why languages like Rust and Julia have robust environment management built directly into the language as a core feature.<br /><br /></p>

<p>For a scientific paper or research project, the following organizational scheme is easy to obtain:<br /></p>

<ul>
  <li>Project
    <ul>
      <li>Package # generalized code for entire project
        <ul>
          <li>Manifest.toml</li>
          <li>Project.toml</li>
          <li>src/</li>
          <li>tests/</li>
          <li>README.md</li>
          <li>…</li>
        </ul>
      </li>
      <li>Figure 1
        <ul>
          <li>Manifest.toml</li>
          <li>Project.toml</li>
          <li>fig1.jl</li>
          <li>data/</li>
        </ul>
      </li>
      <li>Figure 2
        <ul>
          <li>Manifest.toml</li>
          <li>Project.toml</li>
          <li>fig2.jl</li>
          <li>data/</li>
        </ul>
      </li>
      <li>…</li>
    </ul>
  </li>
</ul>

<p>Dependency management is automatic, with no overhead other than the initial creation of a project directory via <code class="language-plaintext highlighter-rouge">Pkg.generate("Project")</code>. To truly compartmentalize one’s work in Python between projects, or even specific analyses within a project, would be practically difficult or impossible. In Julia: enter the project directory and launch Julia. The correct environment with all its dependencies will be loaded from there. The base Julia environment is kept clean.<br /><br /></p>

<h2 id="text-processing">Text Processing</h2>
<p><br />
Computational biologists and bioinformaticians often work with text-based data. Because Julia is JIT compiled and Chars are a first-class type, string data can be processed extremely quickly. BioSequences.jl has efficient minimal representations for biological characters that are intuitive and simple to operate on. For bioinformaticians, this has serious implications: rather than writing difficult to maintain code in C++ or Rust, it is possible to develop a short Julia program (with python-esque syntax) to analyze millions of biological sequences with speed that is comparable to C.<br /><br /></p>

<h3 id="example">Example</h3>
<p><br />
Take a simple program to generate 100M short DNA sequences and check for palindromes. While DNA and RNA can and should be more efficiently represented (Julia has a package that implements efficient representations of DNA/RNA/Protein sequences), lets assume that we are simply optimizing for readability and implementation time.<br /><br /></p>

<h4 id="python">Python</h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">time</span>


<span class="n">complement</span> <span class="o">=</span> <span class="p">{</span><span class="s">'A'</span><span class="p">:</span> <span class="s">'T'</span><span class="p">,</span> <span class="s">'C'</span><span class="p">:</span> <span class="s">'G'</span><span class="p">,</span> <span class="s">'G'</span><span class="p">:</span> <span class="s">'C'</span><span class="p">,</span> <span class="s">'T'</span><span class="p">:</span> <span class="s">'A'</span><span class="p">}</span>

<span class="k">def</span> <span class="nf">ceildiv</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span><span class="p">(</span><span class="n">a</span> <span class="o">//</span> <span class="o">-</span><span class="n">b</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">random_dna_sequence</span><span class="p">(</span><span class="n">n</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="n">letters</span> <span class="o">=</span> <span class="p">[</span><span class="s">"A"</span><span class="p">,</span> <span class="s">"T"</span><span class="p">,</span> <span class="s">"C"</span><span class="p">,</span> <span class="s">"G"</span><span class="p">]</span>
    <span class="k">return</span> <span class="s">''</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">random</span><span class="p">.</span><span class="n">choice</span><span class="p">(</span><span class="n">letters</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">generate_dna_sequences</span><span class="p">(</span><span class="n">count</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">:</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">random_dna_sequence</span><span class="p">(</span><span class="n">random</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">count</span><span class="p">)]</span>

<span class="k">def</span> <span class="nf">is_palindrome</span><span class="p">(</span><span class="n">s</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">False</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ceildiv</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">s</span><span class="p">),</span><span class="mi">2</span><span class="p">)):</span>
            <span class="k">if</span> <span class="n">s</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">!=</span> <span class="n">complement</span><span class="p">[</span><span class="n">s</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="o">-</span> <span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]]:</span>
                <span class="k">return</span> <span class="bp">False</span>
        <span class="k">return</span> <span class="bp">True</span> 

<span class="c1"># Generate 1 million random DNA strings
</span><span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">dna_sequences</span> <span class="o">=</span> <span class="n">generate_dna_sequences</span><span class="p">(</span><span class="mi">100_000_000</span><span class="p">)</span>
<span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Sequence Generation Time: "</span><span class="p">,</span><span class="n">end</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span>

<span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">results</span> <span class="o">=</span> <span class="p">[</span><span class="n">is_palindrome</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">dna_sequences</span><span class="p">]</span>
<span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Palindrome check time: "</span><span class="p">,</span><span class="n">end</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span>

</code></pre></div></div>

<p>Sequence generation took <code class="language-plaintext highlighter-rouge">246.97s</code> and palindrome checking took <code class="language-plaintext highlighter-rouge">12.87s</code>.<br /></p>

<h4 id="julia">Julia</h4>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="kd">const</span> <span class="n">complement</span> <span class="o">=</span> <span class="kt">Dict</span><span class="x">(</span><span class="sc">'A'</span><span class="o">=&gt;</span><span class="sc">'T'</span><span class="x">,</span> <span class="sc">'T'</span><span class="o">=&gt;</span><span class="sc">'A'</span><span class="x">,</span> <span class="sc">'C'</span><span class="o">=&gt;</span><span class="sc">'G'</span><span class="x">,</span> <span class="sc">'G'</span><span class="o">=&gt;</span><span class="sc">'C'</span><span class="x">);</span>

<span class="k">function</span><span class="nf"> is_palindrome</span><span class="x">(</span><span class="n">s</span><span class="o">::</span><span class="kt">Vector</span><span class="x">{</span><span class="kt">Char</span><span class="x">})</span><span class="o">::</span><span class="kt">Bool</span>
    <span class="n">isodd</span><span class="x">(</span><span class="n">length</span><span class="x">(</span><span class="n">s</span><span class="x">))</span> <span class="o">&amp;&amp;</span> <span class="k">return</span> <span class="nb">false</span>

    <span class="nd">@inbounds</span> <span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="mi">1</span><span class="o">:</span><span class="n">cld</span><span class="x">(</span><span class="n">length</span><span class="x">(</span><span class="n">s</span><span class="x">),</span><span class="mi">2</span><span class="x">)</span>
        <span class="n">s</span><span class="x">[</span><span class="n">i</span><span class="x">]</span> <span class="o">!=</span> <span class="n">complement</span><span class="x">[</span><span class="n">s</span><span class="x">[</span><span class="k">end</span><span class="o">-</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="x">]]</span> <span class="o">&amp;&amp;</span> <span class="k">return</span> <span class="nb">false</span>
    <span class="k">end</span>

    <span class="nb">true</span> 
<span class="k">end</span>



<span class="k">function</span><span class="nf"> random_dna_sequence</span><span class="x">(</span><span class="n">n</span><span class="o">::</span><span class="kt">Int</span><span class="x">)</span><span class="o">::</span><span class="kt">Vector</span><span class="x">{</span><span class="kt">Char</span><span class="x">}</span>
    <span class="n">rand</span><span class="x">([</span><span class="sc">'A'</span><span class="x">,</span><span class="sc">'T'</span><span class="x">,</span><span class="sc">'C'</span><span class="x">,</span><span class="sc">'G'</span><span class="x">],</span><span class="n">n</span><span class="x">)</span>
<span class="k">end</span>


<span class="nd">@time</span> <span class="n">nucs</span> <span class="o">=</span> <span class="n">map</span><span class="x">(</span><span class="n">random_dna_sequence</span><span class="x">,</span> <span class="n">rand</span><span class="x">(</span><span class="mi">4</span><span class="o">:</span><span class="mi">10</span><span class="x">,</span><span class="kt">Int</span><span class="x">(</span><span class="mf">1e8</span><span class="x">)))</span>

<span class="nd">@time</span> <span class="n">result</span> <span class="o">=</span> <span class="n">is_palindrome</span><span class="o">.</span><span class="x">(</span><span class="n">nucs</span><span class="x">)</span>


</code></pre></div></div>

<p>Sequence generation took <code class="language-plaintext highlighter-rouge">21.52s</code> and palindrome checking took <code class="language-plaintext highlighter-rouge">1.82s</code>. There are several things to note about the implementations here. The first is that Julia required no imports. Second, Julia’s <code class="language-plaintext highlighter-rouge">@time</code> macro saves a tremendous amount of repetitious code. Third, the <code class="language-plaintext highlighter-rouge">is_palindrome</code> function can be broadcasted over the <code class="language-plaintext highlighter-rouge">nucs</code> vector with the <code class="language-plaintext highlighter-rouge">.</code> syntax. This is despite being a handrolled function, something which is not really feasible in Python. Finally, these functions are so efficient that very little is gained very multithreading. Larger relative improvements in speed would be expected for more complex operations, such as sequence alignment and mapping.</p>

<h2 id="fast-and-easy-multithreading">Fast and Easy Multithreading</h2>
<p><br />
Parallelism and broadcasting in Python are a major weakness of the language, and this is a problem because many if not most bioinformatics workflows are independently parallel. This is an area where Julia truly shines compared to Python. Combined with increased text-based processing speed and native numerical computation, this is when Julia really begins to shine.<br /><br /></p>

<p>In Julia, threading over a loop is as simple as:<br /><br /></p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">using</span> <span class="n">Base</span><span class="o">.</span><span class="n">Threads</span> <span class="c">#brings Threads functions into the namespace, does not need to imported</span>
  
<span class="n">n</span> <span class="o">=</span> <span class="mi">10_000_000</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">zeros</span><span class="x">(</span><span class="kt">Float64</span><span class="x">,</span> <span class="n">n</span><span class="x">)</span>  
<span class="nd">@threads</span> <span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="mi">1</span><span class="o">:</span><span class="n">n</span>
  <span class="n">result</span><span class="x">[</span><span class="n">i</span><span class="x">]</span> <span class="o">=</span> <span class="n">sqrt</span><span class="x">(</span><span class="n">i</span><span class="x">)</span>
<span class="k">end</span>
</code></pre></div></div>
<p><br /><br /></p>

<h2 id="elegant-machine-learning">Elegant Machine Learning</h2>
<p><br />
Julia’s syntax, large scientific and numerical ecosystem, and native support results in elegant code for statistical modeling and machine learning that does not depend on DSLs. Below is a simple program that uses two of Julia’s most powerful packages to yield a MLE for a Gamma distributed sample in only a few lines of code. <br /><br /></p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">using</span> <span class="n">Distributions</span>
<span class="k">using</span> <span class="n">Optim</span>  

<span class="k">function</span><span class="nf"> f</span><span class="x">(</span><span class="n">α</span><span class="x">,</span><span class="n">β</span><span class="x">,</span><span class="n">x</span><span class="x">)</span> <span class="c"># likelihood function</span>
  <span class="o">-</span><span class="n">sum</span><span class="x">(</span><span class="n">logpdf</span><span class="x">(</span><span class="n">Gamma</span><span class="x">(</span><span class="n">exp</span><span class="x">(</span><span class="n">α</span><span class="x">),</span><span class="n">exp</span><span class="x">(</span><span class="n">β</span><span class="x">)),</span> <span class="n">x</span><span class="x">))</span>
<span class="k">end</span>
 
<span class="n">x</span> <span class="o">=</span> <span class="n">rand</span><span class="x">(</span><span class="n">Gamma</span><span class="x">(</span><span class="mf">2.0</span><span class="x">,</span><span class="mf">1.5</span><span class="x">),</span><span class="mi">500</span><span class="x">)</span> <span class="c"># data  </span>
<span class="n">θ_init</span> <span class="o">=</span> <span class="x">[</span><span class="mf">1.0</span><span class="x">,</span><span class="mf">1.0</span><span class="x">]</span>
<span class="n">θ_mle</span><span class="o">=</span> <span class="n">Optim</span><span class="o">.</span><span class="n">optimize</span><span class="x">(</span><span class="n">θ</span> <span class="o">-&gt;</span> <span class="n">f</span><span class="x">(</span><span class="n">θ</span><span class="o">...</span><span class="x">,</span><span class="n">x</span><span class="x">),</span> <span class="n">θ_init</span><span class="x">)</span> <span class="c"># … is the “splat” operator in Julia </span>
</code></pre></div></div>
<p><br /><br />
Line 11 creates a closure such that the optimize routine only performs optimization on the estimated parameters captured in the theta vector. This is a common problem in likelihood functions, as both parameters and data are necessary arguments. Julia treats functions as first-class types, making anonymous functions, closures, and other functional programming paradigms a natural part of the language.<br /><br /></p>

<p>In Julia, it is possible to design custom networks using native Julia code. In early project development, this often helps to quickly iterate on ideas. Significant time is saved from ensuring you understand the behavior of every function in Pytorch/Jax/Tensorflow, which requires substantial upfront time investment.<br /><br /></p>]]></content><author><name></name></author><category term="Julia" /><summary type="html"><![CDATA[]]></summary></entry></feed>